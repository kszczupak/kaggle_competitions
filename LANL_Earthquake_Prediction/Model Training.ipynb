{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "\n",
    "from fastai.datasets import Config\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_path = Config.data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cb\n",
    "import gc\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competiton files setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = base_path/'LANL_Earthquake_Prediction'\n",
    "competition_name = 'LANL-Earthquake-Prediction'\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_prcessed_segments_path = data_path/'train'/'processed_df'\n",
    "test_prcessed_segments_path = data_path/'test_processed_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/train/processed_df/no_overlap_time&freq_features.csv')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prcessed_segments_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/test_processed_df/time&freq_features.csv')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prcessed_segments_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_step_10000_only_time = pd.read_csv(train_prcessed_segments_path/'step_10000_only_time_features.csv')\n",
    "# training_set_no_overlap_only_time = pd.read_csv(train_prcessed_segments_path/'no_overlap_only_time_features.csv')\n",
    "training_set_no_overlap_time_freq = pd.read_csv(train_prcessed_segments_path/'no_overlap_time&freq_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_time_freq = pd.read_csv(test_prcessed_segments_path/'time&freq_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       seg_4743ab\n",
       "1       seg_f86c41\n",
       "2       seg_2d92f0\n",
       "3       seg_1a8f2c\n",
       "4       seg_8509db\n",
       "5       seg_9b7ef8\n",
       "6       seg_c654e7\n",
       "7       seg_d59e4e\n",
       "8       seg_25cca7\n",
       "9       seg_22e509\n",
       "10      seg_407b2b\n",
       "11      seg_57908c\n",
       "12      seg_268249\n",
       "13      seg_15c9f9\n",
       "14      seg_6f2222\n",
       "15      seg_477c83\n",
       "16      seg_32ad0f\n",
       "17      seg_f7050a\n",
       "18      seg_0968f1\n",
       "19      seg_b36650\n",
       "20      seg_61f504\n",
       "21      seg_5254ce\n",
       "22      seg_2c3203\n",
       "23      seg_90c258\n",
       "24      seg_5311d1\n",
       "25      seg_62a403\n",
       "26      seg_d398df\n",
       "27      seg_1ef708\n",
       "28      seg_533613\n",
       "29      seg_218049\n",
       "           ...    \n",
       "2594    seg_41be7d\n",
       "2595    seg_c42490\n",
       "2596    seg_0c89ce\n",
       "2597    seg_c901c0\n",
       "2598    seg_b0a794\n",
       "2599    seg_4c18e2\n",
       "2600    seg_efc5fb\n",
       "2601    seg_fde767\n",
       "2602    seg_c08d36\n",
       "2603    seg_a68007\n",
       "2604    seg_1201e8\n",
       "2605    seg_d516e3\n",
       "2606    seg_f7290f\n",
       "2607    seg_ab2a78\n",
       "2608    seg_6d35cd\n",
       "2609    seg_ff1a62\n",
       "2610    seg_61219c\n",
       "2611    seg_92be9f\n",
       "2612    seg_5467c8\n",
       "2613    seg_ef74dc\n",
       "2614    seg_280863\n",
       "2615    seg_5e7abf\n",
       "2616    seg_572172\n",
       "2617    seg_289d99\n",
       "2618    seg_b378bc\n",
       "2619    seg_4280d9\n",
       "2620    seg_10a595\n",
       "2621    seg_feb312\n",
       "2622    seg_7ce9cb\n",
       "2623    seg_563059\n",
       "Name: name, Length: 2624, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_set_no_overlap_only_time.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "test_set_time_freq['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>...</th>\n",
       "      <th>std_roll_mean_1000</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.430797</td>\n",
       "      <td>4.884113</td>\n",
       "      <td>5.101106</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>74836.577199</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.488552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295715</td>\n",
       "      <td>5.629</td>\n",
       "      <td>3.896</td>\n",
       "      <td>4.072</td>\n",
       "      <td>4.379</td>\n",
       "      <td>5.338</td>\n",
       "      <td>5.484</td>\n",
       "      <td>-1.704698e-06</td>\n",
       "      <td>74222.343443</td>\n",
       "      <td>5.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.427600</td>\n",
       "      <td>4.857127</td>\n",
       "      <td>4.324007</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>74890.988734</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.506360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>5.629</td>\n",
       "      <td>3.896</td>\n",
       "      <td>4.072</td>\n",
       "      <td>4.375</td>\n",
       "      <td>5.320</td>\n",
       "      <td>5.483</td>\n",
       "      <td>-1.906040e-06</td>\n",
       "      <td>74333.053867</td>\n",
       "      <td>5.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.425498</td>\n",
       "      <td>4.837627</td>\n",
       "      <td>5.334216</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>74986.731761</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298291</td>\n",
       "      <td>5.667</td>\n",
       "      <td>3.412</td>\n",
       "      <td>4.069</td>\n",
       "      <td>4.360</td>\n",
       "      <td>5.320</td>\n",
       "      <td>5.483</td>\n",
       "      <td>-6.107383e-07</td>\n",
       "      <td>74474.087926</td>\n",
       "      <td>5.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.423396</td>\n",
       "      <td>4.811160</td>\n",
       "      <td>5.322245</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>74989.309697</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303731</td>\n",
       "      <td>5.667</td>\n",
       "      <td>3.412</td>\n",
       "      <td>4.069</td>\n",
       "      <td>4.360</td>\n",
       "      <td>5.320</td>\n",
       "      <td>5.483</td>\n",
       "      <td>-3.402685e-06</td>\n",
       "      <td>74346.540137</td>\n",
       "      <td>5.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.420198</td>\n",
       "      <td>4.792853</td>\n",
       "      <td>5.418203</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>75145.482449</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.725781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302770</td>\n",
       "      <td>5.667</td>\n",
       "      <td>3.412</td>\n",
       "      <td>4.069</td>\n",
       "      <td>4.359</td>\n",
       "      <td>5.320</td>\n",
       "      <td>5.483</td>\n",
       "      <td>-3.221477e-07</td>\n",
       "      <td>74551.377120</td>\n",
       "      <td>5.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target      mean       std    max    min  mean_change_abs  \\\n",
       "0  1.430797  4.884113  5.101106  104.0  -98.0        -0.000080   \n",
       "1  1.427600  4.857127  4.324007   52.0  -56.0         0.000007   \n",
       "2  1.425498  4.837627  5.334216  181.0 -154.0        -0.000027   \n",
       "3  1.423396  4.811160  5.322245  181.0 -154.0         0.000033   \n",
       "4  1.420198  4.792853  5.418203  181.0 -154.0         0.000027   \n",
       "\n",
       "   mean_change_rate  abs_max  abs_min  std_first_50000  ...  \\\n",
       "0      74836.577199    104.0      0.0         6.488552  ...   \n",
       "1      74890.988734     56.0      0.0         4.506360  ...   \n",
       "2      74986.731761    181.0      0.0         4.680861  ...   \n",
       "3      74989.309697    181.0      0.0         4.748028  ...   \n",
       "4      75145.482449    181.0      0.0         4.725781  ...   \n",
       "\n",
       "   std_roll_mean_1000  max_roll_mean_1000  min_roll_mean_1000  \\\n",
       "0            0.295715               5.629               3.896   \n",
       "1            0.290901               5.629               3.896   \n",
       "2            0.298291               5.667               3.412   \n",
       "3            0.303731               5.667               3.412   \n",
       "4            0.302770               5.667               3.412   \n",
       "\n",
       "   q01_roll_mean_1000  q05_roll_mean_1000  q95_roll_mean_1000  \\\n",
       "0               4.072               4.379               5.338   \n",
       "1               4.072               4.375               5.320   \n",
       "2               4.069               4.360               5.320   \n",
       "3               4.069               4.360               5.320   \n",
       "4               4.069               4.359               5.320   \n",
       "\n",
       "   q99_roll_mean_1000  av_change_abs_roll_mean_1000  \\\n",
       "0               5.484                 -1.704698e-06   \n",
       "1               5.483                 -1.906040e-06   \n",
       "2               5.483                 -6.107383e-07   \n",
       "3               5.483                 -3.402685e-06   \n",
       "4               5.483                 -3.221477e-07   \n",
       "\n",
       "   av_change_rate_roll_mean_1000  abs_max_roll_mean_1000  \n",
       "0                   74222.343443                   5.629  \n",
       "1                   74333.053867                   5.629  \n",
       "2                   74474.087926                   5.667  \n",
       "3                   74346.540137                   5.667  \n",
       "4                   74551.377120                   5.667  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_step_10000_only_time.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "training_set_step_10000_only_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores_path = data_path/'feature_evaluation'/'single_fueature_scores.csv'\n",
    "feature_scores = pd.read_csv(feature_scores_path)\n",
    "# best_22_features = list(feature_scores.sort_values('score')[:22]['feature_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features: 137\n"
     ]
    }
   ],
   "source": [
    "sorted_features = list(feature_scores.sort_values('score')['feature_name'])\n",
    "print(f\"# of features: {len(sorted_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_70_features = sorted_features[:70]\n",
    "best_22_features = sorted_features[:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current training and test sets\n",
    "df_train = training_set_no_overlap_time_freq\n",
    "df_test = test_set_time_freq\n",
    "\n",
    "df_test_segment_names = df_test['name']\n",
    "df_test.drop(['name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q05_roll_std_100</th>\n",
       "      <th>q05_roll_std_1000</th>\n",
       "      <th>q05_roll_std_10</th>\n",
       "      <th>q01_roll_std_100</th>\n",
       "      <th>q01_roll_std_10</th>\n",
       "      <th>q01_roll_std_1000</th>\n",
       "      <th>mad</th>\n",
       "      <th>MA_400MA_std_mean</th>\n",
       "      <th>q95</th>\n",
       "      <th>q95</th>\n",
       "      <th>...</th>\n",
       "      <th>ave_roll_std_100</th>\n",
       "      <th>ave_roll_std_1000</th>\n",
       "      <th>MA_700MA_std_mean</th>\n",
       "      <th>Hilbert_mean</th>\n",
       "      <th>q05</th>\n",
       "      <th>q01</th>\n",
       "      <th>q99</th>\n",
       "      <th>abs_q99</th>\n",
       "      <th>q95_roll_std_1000</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.475639</td>\n",
       "      <td>2.706474</td>\n",
       "      <td>1.636392</td>\n",
       "      <td>2.302634</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>2.616094</td>\n",
       "      <td>3.263401</td>\n",
       "      <td>4.155806</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.050450</td>\n",
       "      <td>4.288590</td>\n",
       "      <td>4.229416</td>\n",
       "      <td>7.027028</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.185756</td>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.475965</td>\n",
       "      <td>2.674879</td>\n",
       "      <td>1.646545</td>\n",
       "      <td>2.300285</td>\n",
       "      <td>1.286684</td>\n",
       "      <td>2.612482</td>\n",
       "      <td>3.574302</td>\n",
       "      <td>4.608579</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.436359</td>\n",
       "      <td>4.843486</td>\n",
       "      <td>4.735352</td>\n",
       "      <td>7.380383</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.544982</td>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.538591</td>\n",
       "      <td>2.761534</td>\n",
       "      <td>1.686548</td>\n",
       "      <td>2.374613</td>\n",
       "      <td>1.316561</td>\n",
       "      <td>2.660178</td>\n",
       "      <td>3.948411</td>\n",
       "      <td>5.122560</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.917334</td>\n",
       "      <td>5.423013</td>\n",
       "      <td>5.283180</td>\n",
       "      <td>8.016930</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.845834</td>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.496442</td>\n",
       "      <td>2.716991</td>\n",
       "      <td>1.649916</td>\n",
       "      <td>2.330539</td>\n",
       "      <td>1.269296</td>\n",
       "      <td>2.624962</td>\n",
       "      <td>3.647117</td>\n",
       "      <td>4.698889</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.533343</td>\n",
       "      <td>4.939280</td>\n",
       "      <td>4.826369</td>\n",
       "      <td>7.606850</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.715642</td>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.491521</td>\n",
       "      <td>2.719174</td>\n",
       "      <td>1.646545</td>\n",
       "      <td>2.314731</td>\n",
       "      <td>1.269296</td>\n",
       "      <td>2.628699</td>\n",
       "      <td>3.826052</td>\n",
       "      <td>4.910515</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.761149</td>\n",
       "      <td>5.121868</td>\n",
       "      <td>5.023478</td>\n",
       "      <td>7.895403</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.923676</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   q05_roll_std_100  q05_roll_std_1000  q05_roll_std_10  q01_roll_std_100  \\\n",
       "0          2.475639           2.706474         1.636392          2.302634   \n",
       "1          2.475965           2.674879         1.646545          2.300285   \n",
       "2          2.538591           2.761534         1.686548          2.374613   \n",
       "3          2.496442           2.716991         1.649916          2.330539   \n",
       "4          2.491521           2.719174         1.646545          2.314731   \n",
       "\n",
       "   q01_roll_std_10  q01_roll_std_1000       mad  MA_400MA_std_mean   q95  \\\n",
       "0         1.264911           2.616094  3.263401           4.155806  11.0   \n",
       "1         1.286684           2.612482  3.574302           4.608579  12.0   \n",
       "2         1.316561           2.660178  3.948411           5.122560  13.0   \n",
       "3         1.269296           2.624962  3.647117           4.698889  12.0   \n",
       "4         1.269296           2.628699  3.826052           4.910515  12.0   \n",
       "\n",
       "    q95  ...  ave_roll_std_100  ave_roll_std_1000  MA_700MA_std_mean  \\\n",
       "0  11.0  ...          4.050450           4.288590           4.229416   \n",
       "1  12.0  ...          4.436359           4.843486           4.735352   \n",
       "2  13.0  ...          4.917334           5.423013           5.283180   \n",
       "3  12.0  ...          4.533343           4.939280           4.826369   \n",
       "4  12.0  ...          4.761149           5.121868           5.023478   \n",
       "\n",
       "   Hilbert_mean  q05   q01   q99  abs_q99  q95_roll_std_1000    target  \n",
       "0      7.027028 -2.0  -8.0  18.0     20.0           8.185756  1.430797  \n",
       "1      7.380383 -2.0 -11.0  21.0     24.0          10.544982  1.391499  \n",
       "2      8.016930 -3.0 -15.0  26.0     30.0          14.845834  1.353196  \n",
       "3      7.606850 -2.0 -12.0  22.0     26.0          11.715642  1.313798  \n",
       "4      7.895403 -2.0 -15.0  26.0     32.0          13.923676  1.274400  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test[best_22_features]\n",
    "best_22_features.append('target')\n",
    "df_train = df_train[best_22_features]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch only train set\n",
    "df_train = training_set_step_10000_only_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 4178\n",
      "Test length: 2624\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train length: {len(df_train)}\")\n",
    "print(f\"Test length: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LightGBM as a model to predict target for test dataset. This is a tree-based model, which cannot see frequency of the feature (number of occurrences of particular value in a feature). It turns out that this is important in this case, so frequencies needs to be added as separate feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## 1st Layer of ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train.drop('target', axis=1)\n",
    "targets = df_train['target']\n",
    "\n",
    "## Scaling\n",
    "scaler.fit(train_data)\n",
    "train_data = pd.DataFrame(scaler.transform(train_data))\n",
    "df_test = pd.DataFrame(scaler.transform(df_test))\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'learning_rate': 0.055,\n",
    "    'num_leaves': 8,\n",
    "    'metric':'mean_absolute_error',\n",
    "    'boost_from_average':'false',\n",
    "    'feature_fraction': 0.7, # This is important; we must use ALL features in every iteration to make sure that feature freq will be used\n",
    "    'max_depth': 3,\n",
    "    'objective': 'regression',\n",
    "#     'verbosity': -10\n",
    "    'boosting': 'gbdt',\n",
    "    'lambda_l1': 0.45,\n",
    "    'lambda_l2': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = lgbm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: 2.0322617120741273\n",
      "CPU times: user 31.7 s, sys: 112 ms, total: 31.8 s\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nbr_of_folds = 10\n",
    "nbr_of_rounds = 10000\n",
    "\n",
    "folds = KFold(\n",
    "    n_splits=nbr_of_folds,\n",
    "    shuffle=True,\n",
    "    random_state=2019\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "out_of_fold_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "\n",
    "\n",
    "folds_predictions = np.zeros(len(df_test))\n",
    "training_results = dict()\n",
    "\n",
    "for fold_nbr, (train_idx, valid_idx) in enumerate(folds.split(train_data, targets)):\n",
    "    fold_train_data = train_data.iloc[train_idx]\n",
    "    fold_train_target = targets.iloc[train_idx]\n",
    "        \n",
    "    fold_valid_data = train_data.iloc[valid_idx]\n",
    "    fold_valid_target = targets.iloc[valid_idx]\n",
    "\n",
    "    lgb_fold_train = lgb.Dataset(fold_train_data, label=fold_train_target)\n",
    "    lgb_fold_valid = lgb.Dataset(fold_valid_data, label=fold_valid_target, reference=lgb_fold_train)\n",
    "\n",
    "    model = lgb.train(\n",
    "        param, \n",
    "        lgb_fold_train, \n",
    "        nbr_of_rounds, \n",
    "        valid_sets=[lgb_fold_train, lgb_fold_valid], \n",
    "       early_stopping_rounds=500,\n",
    "        verbose_eval=False, \n",
    "        evals_result=training_results,\n",
    "#        verbose_eval=5000\n",
    "    )\n",
    "    \n",
    "#     print(training_results)\n",
    "#     scores = training_results['valid_1']['l1']\n",
    "#     best_score = min(scores)\n",
    "#     best_round = scores.index(min(scores))\n",
    "#     print(f\"Fold #{fold_nbr}: Best l1: {best_score} ({best_round} iteration)\")\n",
    "\n",
    "    out_of_fold_predictions[valid_idx] = model.predict(fold_valid_data) #, num_iteration=best_round)\n",
    "    folds_predictions += model.predict(df_test) #, num_iteration=best_round)\n",
    "\n",
    "test_predictions = folds_predictions/nbr_of_folds\n",
    "\n",
    "print(f\"Final score: {mean_absolute_error(targets.values, out_of_fold_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.19969 , 4.450899, 9.509817, 7.613244, ..., 5.66776 , 6.436429, 4.000546, 6.882363])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.663420702760496"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best 22 features\n",
    "\n",
    "Without scaler: 2.07108839172104 (LB 1.614)\n",
    "With scaler: 2.0674590671564608 \n",
    "\n",
    "Best 70 features\n",
    "\n",
    "Without scaler: 2.0766704829894915 (LB 1.604)\n",
    "With scaler: 2.0746327066782286\n",
    "\n",
    "All features\n",
    "\n",
    "Without scaler:  2.0640357531770532 (LB 1.554?)\n",
    "With scaler: 2.0634338328764064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL HYPEROPT PARAMETERS\n",
    "NUM_EVALS = 1000 #number of hyperopt evaluation rounds\n",
    "N_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n",
    "\n",
    "#LIGHTGBM PARAMETERS\n",
    "LGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\n",
    "LGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\n",
    "EVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \n",
    "EVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n",
    "\n",
    "#XGBOOST PARAMETERS\n",
    "XGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\n",
    "XGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\n",
    "EVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\n",
    "EVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n",
    "\n",
    "#CATBOOST PARAMETERS\n",
    "CB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\n",
    "OBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\n",
    "OBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n",
    "\n",
    "#OPTIONAL OUTPUT\n",
    "BEST_SCORE = 0\n",
    "\n",
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                         'num_leaves',\n",
    "                          'max_bin',\n",
    "                         'min_data_in_leaf',\n",
    "                         'min_data_in_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n",
    "                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = lgb.Dataset(data, labels)\n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_reg\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n",
    "                             early_stopping_rounds=100, stratified=False, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [{'tree_method' : 'exact'},\n",
    "               {'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_reg\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : -1\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #CatBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='cb':\n",
    "        \n",
    "        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n",
    "        \n",
    "        #clear memory \n",
    "        gc.collect()\n",
    "            \n",
    "        integer_params = ['depth',\n",
    "                          #'one_hot_max_size', #for categorical data\n",
    "                          'min_data_in_leaf',\n",
    "                          'max_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "                        \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n",
    "                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n",
    "                space_params['bagging_temperature'] = bagging_temp\n",
    "                \n",
    "            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n",
    "                max_leaves = space_params['grow_policy'].get('max_leaves')\n",
    "                space_params['max_leaves'] = int(max_leaves)\n",
    "                \n",
    "            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n",
    "            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n",
    "                           \n",
    "            #random_strength cannot be < 0\n",
    "            space_params['random_strength'] = max(space_params['random_strength'], 0)\n",
    "            #fold_len_multiplier cannot be < 1\n",
    "            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n",
    "                       \n",
    "            #for classification set stratified=True\n",
    "            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n",
    "                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n",
    "           \n",
    "            best_loss = cv_results['test-MAE-mean'].iloc[-1] #'test-RMSE-mean' for RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            \n",
    "            return{'loss':best_loss, 'status': STATUS_OK}\n",
    "        \n",
    "        train = cb.Pool(data, labels.astype('float32'))\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        bootstrap_type = [{'bootstrap_type':'Poisson'}, \n",
    "                           {'bootstrap_type':'Bayesian',\n",
    "                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n",
    "                          {'bootstrap_type':'Bernoulli'}] \n",
    "        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n",
    "        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n",
    "        grow_policy = [{'grow_policy':'SymmetricTree'},\n",
    "                       {'grow_policy':'Depthwise'},\n",
    "                       {'grow_policy':'Lossguide',\n",
    "                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n",
    "        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n",
    "        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n",
    "        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n",
    "        eval_metric_list = eval_metric_list_reg\n",
    "                \n",
    "        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n",
    "                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n",
    "                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n",
    "                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n",
    "                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n",
    "                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n",
    "                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n",
    "                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n",
    "                'objective' : OBJECTIVE_CB_REG,\n",
    "                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n",
    "                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n",
    "                'grow_policy': hp.choice('grow_policy', grow_policy),\n",
    "                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n",
    "                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "                'od_type' : 'Iter',\n",
    "                'od_wait' : 25,\n",
    "                'task_type' : 'GPU',\n",
    "                'verbose' : 0\n",
    "            }\n",
    "        \n",
    "        #optional: run CatBoost without GPU\n",
    "        #uncomment line below\n",
    "        #space['task_type'] = 'CPU'\n",
    "            \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        #unpack nested dicts first\n",
    "        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n",
    "        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n",
    "        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n",
    "        \n",
    "        #best['score_function'] = score_function[best['score_function']] \n",
    "        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n",
    "        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    else:\n",
    "        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2500 rounds of LightGBM parameter optimisation:\n",
      "100%|██████████| 2500/2500 [46:54<00:00,  1.55s/it, best loss: 2.024108230634918] \n",
      "{bagging_fraction: 0.6900000000000001\n",
      "boosting: goss\n",
      "feature_fraction: 0.73\n",
      "lambda_l1: 2.405766598405247\n",
      "lambda_l2: 1.8511298264546527\n",
      "learning_rate: 0.1747309888380998\n",
      "max_bin: 218\n",
      "max_depth: 16\n",
      "metric: RMSE\n",
      "min_data_in_bin: 113\n",
      "min_data_in_leaf: 28\n",
      "min_gain_to_split: 3.25\n",
      "num_leaves: 431\n",
      "objective: huber\n",
      "other_rate: 0.27957018732110944\n",
      "top_rate: 0.03663841459503298}\n"
     ]
    }
   ],
   "source": [
    "data = df_train.drop('target', axis=1)\n",
    "labels = df_train['target']\n",
    "lgbm_params = quick_hyperopt(data, labels, 'lgbm', 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Layer of ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "    out_of_fold_predictions, \n",
    "    targets.values, \n",
    "    test_size=0.15, \n",
    "    random_state=2019\n",
    ")\n",
    "\n",
    "ensamble_second_layer = LogisticRegression(n_jobs=-1)\n",
    "ensamble_second_layer.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9040947053429751"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = ensamble_second_layer.predict_proba(X_valid)\n",
    "roc_auc_score(Y_valid, probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Combined Model with magic Val_AUC= 0.91402\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "logr = sm.Logit(targets, out_of_fold_predictions)\n",
    "logr = logr.fit(disp=0)\n",
    "ensemble_preds = logr.predict(out_of_fold_predictions)\n",
    "ensemble_auc = roc_auc_score(targets, ensemble_preds)  \n",
    "print('##################')\n",
    "print('Combined Model with magic Val_AUC=',round(ensemble_auc,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's auc: 0.893676\tvalid_1's auc: 0.878561\n",
      "[2000]\ttraining's auc: 0.912475\tvalid_1's auc: 0.895864\n",
      "[3000]\ttraining's auc: 0.921207\tvalid_1's auc: 0.902603\n",
      "[4000]\ttraining's auc: 0.926821\tvalid_1's auc: 0.906273\n",
      "[5000]\ttraining's auc: 0.931289\tvalid_1's auc: 0.908226\n",
      "[6000]\ttraining's auc: 0.935035\tvalid_1's auc: 0.909387\n",
      "[7000]\ttraining's auc: 0.938219\tvalid_1's auc: 0.910122\n",
      "[8000]\ttraining's auc: 0.94106\tvalid_1's auc: 0.910847\n",
      "[9000]\ttraining's auc: 0.943547\tvalid_1's auc: 0.911536\n",
      "[10000]\ttraining's auc: 0.945812\tvalid_1's auc: 0.912033\n",
      "[11000]\ttraining's auc: 0.947898\tvalid_1's auc: 0.912439\n",
      "[12000]\ttraining's auc: 0.949817\tvalid_1's auc: 0.912743\n",
      "[13000]\ttraining's auc: 0.951595\tvalid_1's auc: 0.912847\n",
      "Early stopping, best iteration is:\n",
      "[12584]\ttraining's auc: 0.950872\tvalid_1's auc: 0.91289\n"
     ]
    }
   ],
   "source": [
    "lgb_ensemble_train = lgb.Dataset(X_train, label=Y_train)\n",
    "lgb_ensemble_valid = lgb.Dataset(X_valid, label=Y_valid, reference=lgb_ensemble_train)\n",
    "\n",
    "model = lgb.train(\n",
    "    param, \n",
    "    lgb_ensemble_train, \n",
    "    50000, \n",
    "    valid_sets=[lgb_ensemble_train, lgb_ensemble_valid], \n",
    "    early_stopping_rounds=500,\n",
    "#     verbose_eval=False, \n",
    "#             evals_result=training_results\n",
    "    verbose_eval=1000\n",
    "    )\n",
    "\n",
    "# ensemble_predicions = model.predict(fold_valid_data)\n",
    "# folds_predictions += model.predict(df_test, num_iteration=best_round)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction and submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'seg_id': df_test_segment_names, 'time_to_failure': test_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_4743ab</td>\n",
       "      <td>7.199690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_f86c41</td>\n",
       "      <td>4.450899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_2d92f0</td>\n",
       "      <td>9.509817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_1a8f2c</td>\n",
       "      <td>7.613244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_8509db</td>\n",
       "      <td>5.202186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id  time_to_failure\n",
       "0  seg_4743ab         7.199690\n",
       "1  seg_f86c41         4.450899\n",
       "2  seg_2d92f0         9.509817\n",
       "3  seg_1a8f2c         7.613244\n",
       "4  seg_8509db         5.202186"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_name = \"time_and_freq_features_new_parameters_fixed\"\n",
    "\n",
    "submission_path = data_path/'submissions'\n",
    "submission_path.mkdir(exist_ok=True)\n",
    "submission_file = submission_path/f\"{submission_file_name}.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 74.6k/74.6k [00:08<00:00, 8.78kB/s]\n",
      "403 - Your team has used its submission allowance (2 of 2). This resets at midnight UTC (2.1 hours from now).\n"
     ]
    }
   ],
   "source": [
    "# Only 2 submission allowed per day!\n",
    "!kaggle competitions submit {competition_name} -f {submission_file} -m \"LGBM (optimized, fixed scaling), all time and freq features, with scaling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_sandbox]",
   "language": "python",
   "name": "conda-env-kaggle_sandbox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
