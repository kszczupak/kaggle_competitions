{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators # time series feature exctraction\n",
    "import librosa # music/aucustic signal manipulation, used here to mfcc feature exctraction\n",
    "import pywt # wavelet transform library\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "from fastai.datasets import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = Config.data_path()\n",
    "data_path = base_path/'LANL_Earthquake_Prediction'\n",
    "competition_name = 'LANL-Earthquake-Prediction'\n",
    "(data_path/'temp').mkdir(exist_ok=True)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/submissions'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/train'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/test'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/test_processed_df'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/train.csv'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/test.zip'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/sample_submission.csv'),\n",
       " PosixPath('/home/krzysiek/.fastai/data/LANL_Earthquake_Prediction/temp')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 32.7 s, total: 2min 23s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load training samples\n",
    "raw = pd.read_csv(\n",
    "    data_path/'train.csv', \n",
    "    dtype={\n",
    "        'acoustic_data': np.int16, \n",
    "        'time_to_failure': np.float64\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "noise = np.random.normal(0, 0.5, 150_000)\n",
    "\n",
    "def denoise_signal_simple(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    #univeral threshold\n",
    "    uthresh = 10\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "def feature_gen(z):\n",
    "    X = pd.DataFrame(index=[0], dtype=np.float64)\n",
    "    \n",
    "    z = z + noise\n",
    "    z = z - np.median(z)\n",
    "\n",
    "    den_sample_simple = denoise_signal_simple(z)\n",
    "    mfcc = librosa.feature.mfcc(z)\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    percentile_roll50_std_20 = np.percentile(pd.Series(z).rolling(50).std().dropna().values, 20)\n",
    "    \n",
    "    X['var_num_peaks_2_denoise_simple'] = feature_calculators.number_peaks(den_sample_simple, 2)\n",
    "    X['var_percentile_roll50_std_20'] = percentile_roll50_std_20\n",
    "    X['var_mfcc_mean18'] = mfcc_mean[18]\n",
    "    X['var_mfcc_mean4'] = mfcc_mean[4]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def parse_sample(sample, start):\n",
    "    delta = feature_gen(sample['acoustic_data'].values)\n",
    "    delta['start'] = start\n",
    "    delta['target'] = sample['time_to_failure'].values[-1]\n",
    "    return delta\n",
    "    \n",
    "def sample_train_gen(df, segment_size=150_000, indices_to_calculate=[0]):\n",
    "    result = Parallel(n_jobs=4, temp_folder=data_path/'temp', max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample)(df[int(i) : int(i) + segment_size], int(i)) \n",
    "                                                                                                for i in tqdm(indices_to_calculate))\n",
    "    data = [r.values for r in result]\n",
    "    data = np.vstack(data)\n",
    "    X = pd.DataFrame(data, columns=result[0].columns)\n",
    "    X = X.sort_values(\"start\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "def parse_sample_test(seg_id):\n",
    "    sample = pd.read_csv(data_path/'test'/f\"{seg_id}.csv\", dtype={'acoustic_data': np.int32})\n",
    "    delta = feature_gen(sample['acoustic_data'].values)\n",
    "    delta['seg_id'] = seg_id\n",
    "    return delta\n",
    "\n",
    "def sample_test_gen():\n",
    "    X = pd.DataFrame()\n",
    "    submission = pd.read_csv(data_path/'sample_submission.csv', index_col='seg_id')\n",
    "    result = Parallel(n_jobs=4, temp_folder=data_path/'temp', max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample_test)(seg_id) for seg_id in tqdm(submission.index))\n",
    "    data = [r.values for r in result]\n",
    "    data = np.vstack(data)\n",
    "    X = pd.DataFrame(data, columns=result[0].columns)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036c4723620048a3b885283303a084b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4194), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2533f77e77412ba2aa3c1f7941e141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices_to_calculate = raw.index.values[::150_000][:-1]\n",
    "\n",
    "train = sample_train_gen(raw, indices_to_calculate=indices_to_calculate)\n",
    "gc.collect()\n",
    "test = sample_test_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "etq_meta = [\n",
    "    {\"start\":0,         \"end\":5656574},\n",
    "    {\"start\":5656574,   \"end\":50085878},\n",
    "    {\"start\":50085878,  \"end\":104677356},\n",
    "    {\"start\":104677356, \"end\":138772453},\n",
    "    {\"start\":138772453, \"end\":187641820},\n",
    "    {\"start\":187641820, \"end\":218652630},\n",
    "    {\"start\":218652630, \"end\":245829585},\n",
    "    {\"start\":245829585, \"end\":307838917},\n",
    "    {\"start\":307838917, \"end\":338276287},\n",
    "    {\"start\":338276287, \"end\":375377848},\n",
    "    {\"start\":375377848, \"end\":419368880},\n",
    "    {\"start\":419368880, \"end\":461811623},\n",
    "    {\"start\":461811623, \"end\":495800225},\n",
    "    {\"start\":495800225, \"end\":528777115},\n",
    "    {\"start\":528777115, \"end\":585568144},\n",
    "    {\"start\":585568144, \"end\":621985673},\n",
    "    {\"start\":621985673, \"end\":629145480},\n",
    "]\n",
    "\n",
    "for i, etq in enumerate(etq_meta):\n",
    "    train.loc[(train['start'] + 150_000 >= etq[\"start\"]) & (train['start'] <= etq[\"end\"] - 150_000), \"eq\"] = i\n",
    "\n",
    "train_sample = train[train[\"eq\"].isin([2, 7, 0, 4, 11, 13, 9, 1, 14, 10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:   6.258\n",
      "Median: 6.031\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean:   {train_sample['target'].mean():.4}\")\n",
    "print(f\"Median: {train_sample['target'].median():.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.59127\tvalid_1's l1: 1.97283\n",
      "Early stopping, best iteration is:\n",
      "[587]\ttraining's l1: 1.66628\tvalid_1's l1: 1.95632\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.63939\tvalid_1's l1: 1.86458\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's l1: 1.88729\tvalid_1's l1: 1.83689\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.62559\tvalid_1's l1: 1.92958\n",
      "Early stopping, best iteration is:\n",
      "[391]\ttraining's l1: 1.74828\tvalid_1's l1: 1.90434\n",
      "\n",
      "MAE:  1.8992052473248857\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "features = ['var_num_peaks_2_denoise_simple','var_percentile_roll50_std_20','var_mfcc_mean4',  'var_mfcc_mean18']\n",
    "target = train_sample[\"target\"].values\n",
    "\n",
    "train_X = train_sample[features].values\n",
    "test_X = test[features].values\n",
    "\n",
    "submission = pd.read_csv(data_path/'sample_submission.csv', index_col='seg_id')\n",
    "oof = np.zeros(len(train_X))\n",
    "prediction = np.zeros(len(submission))\n",
    "\n",
    "n_fold = 3\n",
    "\n",
    "kf = KFold(n_splits=n_fold, shuffle=True, random_state=1337)\n",
    "kf = list(kf.split(np.arange(len(train_sample))))\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf):\n",
    "    print('Fold', fold_n)\n",
    "\n",
    "    trn_data = lgb.Dataset(train_X[train_index], label=target[train_index])\n",
    "    val_data = lgb.Dataset(train_X[valid_index], label=target[valid_index])\n",
    "    \n",
    "    params = {'num_leaves': 4,\n",
    "      'min_data_in_leaf': 5,\n",
    "      'objective':'fair',\n",
    "      'max_depth': -1,\n",
    "      'learning_rate': 0.02,\n",
    "      \"boosting\": \"gbdt\",\n",
    "      'boost_from_average': True,\n",
    "      \"feature_fraction\": 0.9,\n",
    "      \"bagging_freq\": 1,\n",
    "      \"bagging_fraction\": 0.5,\n",
    "      \"bagging_seed\": 0,\n",
    "      \"metric\": 'mae',\n",
    "      \"verbosity\": -1,\n",
    "      'max_bin': 500,\n",
    "      'reg_alpha': 0,\n",
    "      'reg_lambda': 0,\n",
    "      'seed': 0,\n",
    "      'n_jobs': 1\n",
    "      }\n",
    "\n",
    "    clf = lgb.train(params, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "\n",
    "    oof[valid_index] += clf.predict(train_X[valid_index], num_iteration=clf.best_iteration)\n",
    "    prediction += clf.predict(test_X, num_iteration=clf.best_iteration)\n",
    "\n",
    "prediction /= n_fold\n",
    "\n",
    "print('\\nMAE: ', mean_absolute_error(target, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time_to_failure\n",
      "seg_id                     \n",
      "seg_00030f         4.188369\n",
      "seg_0012b5         5.778676\n",
      "seg_00184e         7.223985\n",
      "seg_003339        10.427658\n",
      "seg_0042cc         7.773074\n"
     ]
    }
   ],
   "source": [
    "submission['time_to_failure'] = prediction \n",
    "submission_file_name = \"winning_solution\"\n",
    "\n",
    "submission_path = data_path/'submissions'\n",
    "submission_path.mkdir(exist_ok=True)\n",
    "submission_file = submission_path/f\"{submission_file_name}.csv\"\n",
    "\n",
    "submission.to_csv(submission_file, line_terminator=os.linesep)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 75.0k/75.0k [00:02<00:00, 27.0kB/s]\n",
      "Successfully submitted to LANL Earthquake Prediction"
     ]
    }
   ],
   "source": [
    "# Only 2 submission allowed per day!\n",
    "!kaggle competitions submit {competition_name} -f {submission_file} -m \"Winning solution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_sandbox]",
   "language": "python",
   "name": "conda-env-kaggle_sandbox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
